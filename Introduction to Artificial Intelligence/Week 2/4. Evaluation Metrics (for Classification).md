Previous: [[3. Key Concepts and Methods]]

We use a **Confusion Matrix** to understand a model's performance, not just its accuracy.

|                      | **Predicted: SPAM** | **Predicted: NOT SPAM** |
| -------------------- | ------------------- | ----------------------- |
| **Actual: SPAM**     | True Positive (TP)  | False Negative (FN)     |
| **Actual: NOT SPAM** | False Positie (FP)  | True Negative (TN)      |
**Example:** 200 emails total (5 are *actually* spam). A filter labels 6 emails as spam. 1 is correct (TP), but 5 are not (FP).
- **Confusion Matrix:**
	- TP: 1 (spam caught)
	- FN: 4 (spam missed)
	- FP: 5 (not-spam flagged)
	- TN: 190 (not-spam correctly ignored)
- **Accuracy:** (TP + TN) / total
	- (1 + 190) / 200 = 95.5%. This *looks* good, but is misleading!
- **Precision:** TP / (TP + FP)
	- 1 / (1 + 5) = 16.7%. This is the "Positive Predictive Value". It tells you: *of the emails we flagged as spam, only 16.7% were actually spam.*
- **Recall:** TP / (TP + FN)
	- 1 / (1 + 4) = 20%. This is the "sensitivity". It tells you: *we only caught 20% of the total spam emails.*
- **F1-Score:** the harmonic mean of Precision and Recall, providing a single balanced score.

Next up: [[5. Key Models - an Introduction]]