Previous: [[3. Training Deep Networks]]
# Convolutional Neural Networks (CNN)
Best for high-dimensional grid data like **images** or audio.
- **Concept:** instead of connecting every pixel to every neuron (fully connected), CNNs use **Kernels** (filters) that slide over the input (Convolution).
- **Local information:** this captures local patterns (neighbours matter) efficiently with fewer parameters.
- **Pooling/downsampling:** layers (like Max Pooling) that reduce the signal size, extracting dominant features and allowing subsequent layers to see a wider area (receptive field).
- **Hourglass Architectures:** combine downsampling (encoding) and upsampling (decoding) with **skip connections** to solve tasks like segmentation.

# Recurrent Neural Networks (RNN)
Best for sequential data like **text** or time-series.
- **Concept:** neurons have backward connections to their own inputs, creating a "memory" of previous states.
- **Issue:** very sensitive to vanishing/exploding gradients over long sequences.

# LSTMs & Transformers
- **LSTM (Long Short-Term Memory):** adds internal "gates" (input, output, forget) to RNN cells, allowing them to learn when to remember or forget information over long periods.
- **Transformers:** the foundation of modern AI (like GPT). They use a **Attention** mechanism to determine which parts of the input are relevant to each other, without recurrent connection. They are highly parallelizable and powerful.

Next up: [[5. Case Study - Deep Learning for Diabetes (PIMA)]]