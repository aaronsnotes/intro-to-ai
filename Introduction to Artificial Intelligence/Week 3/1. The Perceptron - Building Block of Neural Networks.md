Back to index: [[Intro to AI - Index]]
# Concept
The perceptron is the basic unit of artificial neural networks, inspired by biological neurons. It computes a weighted sum of inputs and passes the result through an activation function.
- **Formula:** $a = g(w^Tx)$
- **Inputs:** $x$<sub>1</sub>$...x$<sub>n</sub> (with $x$<sub>0</sub> usually fixed to 1 for bias).
- **Weights:** $w$<sub>0</sub>$...w$<sub>n</sub>.
- **Activation function:** $g()$.
- **Output:** $a$.
If the activation function $g()$ is the **identity function**, a single perceptron corresponds to a **[[5. Key Models - an Introduction|linear regression model]].**

# Activation functions
The choice of activation function determines the network's capacity to learn complex patterns. Common examples include:
- **Logistic / Sigmoid:** $g(x) = \frac{1}{1+e^{-x}}$. Outputs are between 0 and 1, useful for classification.
- **Hyperbolic Tangent (tanh):** $g(x) = \frac{e^{2x}-1}{e^{2x}+1}$. Outputs are between -1 and 1, centered around 0 4.
- **Rectified Linear Unit (ReLU):** $g(x) = max(0, x)$. Very common. Nullifies negative values, which saves computation but can "kill" neurons (stop them from learning). Variations like Leaky ReLU exist to fix this 5.
- **Softplus:** $g(x) = log(1+e^x)$. A smooth approximation of ReLU.

Next up: [[2. Multi-Layer Perceptrons (MLP)]]