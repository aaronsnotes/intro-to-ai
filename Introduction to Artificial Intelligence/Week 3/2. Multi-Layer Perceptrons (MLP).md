Previous: [[1. The Perceptron - Building Block of Neural Networks]]
# Architecture
- **Layers:** inputs connect to outputs through layers of perceptrons.
- **Feed-forward:** data flows in one direction (input -> hidden layers -> output).
- **Hidden layers:** layers not directly connected to input or output. Having at least two hidden layers is the classical definition of **deep learning**.
- **Fully connected:** every input of a layer connects to every unit in that layer.

# Encoding and feature space
Deep networks work by transforming data through multiple layers into a **abstract feature space**.
- **Encoder part:** transforms/compresses data into a space where the task is easier to solve.
- **Decoder part:** interprets the encoded data back into the desired output space.
- **Why deep learning works:** chaining enough transformations allows the network to approximate any function needed to solve the task.

Next up: [[3. Training Deep Networks]]