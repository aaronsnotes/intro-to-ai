Previous: [[2. Multi-Layer Perceptrons (MLP)]]
# Stochastic Gradient Descent (SGD)
Because deep networks have huge numbers of parameters, standard gradient descent is too slow.
- **SGD:** computes gradients and updates parameters using only a small, random subset of data (**mini-batch**) at a time.
- **Benefits:** faster convergence and better generalisation.

# Back-propagation
To train effectively, gradients are computed layer-by-layer, starting from the output and moving backward.
- **Vanishing gradients:** a major issue where gradients become very small in earlier layers, preventing them from learning effectively.

# Improving generalisation
We want models to perform well on new, unseen data, not just memorise the training set.
- **Weight decay:** a regularisation term added to the loss function that penalises large weights, forcing the network to learn simpler models.
- **Dropout:** randomly deactivating some neurons during training. This forces the network not to rely on specific neurons, making it more robust (like training an ensemble of models).
- **Batch normalisation:** rescaling inputs/outputs of neurons to have mean 0 and variance 1. This stabilises training and helps prevent vanishing gradients.
- **Early stopping:** monitoring the **Validation Set** loss during training. Stop training when the validation error starts to increase, which indicates overfitting.

Next up: [[4. Modern Architectures]]